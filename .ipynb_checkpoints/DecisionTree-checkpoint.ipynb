{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import treePlotter\n",
    "import numpy as np\n",
    "\n",
    "class DecisionClassifier(object):\n",
    "   \n",
    "    def __init__(self, max_depth = 7, criterion = \"id3\", eps = 1e-2):\n",
    "        \n",
    "        if not isinstance(max_depth, int) or not max_depth > 0:\n",
    "            raise Exception(\"max_depth need to bigger than 0 and the type of it must be int.Please check it.\")\n",
    "        if criterion not in (\"id3\", \"c4.5\", \"gini\"):\n",
    "            raise Exception(\"criterion must set 'id3' or 'c4.5' or 'gini'.The type of it is str.Please check it.\")\n",
    "        if not isinstance(eps, float):\n",
    "            raise Exception(\"The type of eps must be float,please check it.\")\n",
    "        self.max_depth = max_depth\n",
    "        self.criterion = criterion\n",
    "        self.eps = eps\n",
    "\n",
    "    def fit(self, X, y, show_graph = False):\n",
    "        \"\"\"\n",
    "        fit():\n",
    "            parameters:\n",
    "                1. X: the feature matrix of training set.\n",
    "                    dype:DataFrame\n",
    "                2. y: corresponding label to training set.\n",
    "                    dtype:DataFrame\n",
    "                3. show_graph: if this parameter is true, the structure of the tree will be visualized.\n",
    "                    dtype:bool\n",
    "        \"\"\"\n",
    "     \n",
    "        all_features = list(X.columns)\n",
    "        \n",
    "        self.__all_feature_dict = dict([( feature, X[feature].unique() ) for feature in all_features])\n",
    "        \n",
    "        self.tree = self.__createTree(X = X, y = y, candidate_features = all_features)\n",
    "        \n",
    "        \n",
    "        if show_graph == True:\n",
    "            treePlotter.createPlot(self.tree)\n",
    "        \n",
    "\n",
    "    def __createTree(self, X, y, candidate_features, cur_depth = 1):\n",
    "      \n",
    "       \n",
    "        if cur_depth >= self.max_depth:\n",
    "            return y.iloc[np.argmax(y.value_counts())]\n",
    "        \n",
    "        if y.unique().shape[0] == 1:\n",
    "            return y.iloc[0]\n",
    "      \n",
    "        if candidate_features == []:\n",
    "            return y.iloc[np.argmax(y.value_counts())]\n",
    "       \n",
    "        best_split_feature, biggest_gain = self.__select_best_split_feature(X, y, candidate_features)\n",
    "       \n",
    "        if biggest_gain < self.eps:\n",
    "            return y.iloc[np.argmax(y.value_counts())]\n",
    "        \n",
    "        tree = {best_split_feature: {}}\n",
    "       \n",
    "        for best_feature_value in self.__all_feature_dict[best_split_feature]:\n",
    "       \n",
    "            index = X[ X[best_split_feature] == best_feature_value ].index\n",
    "\n",
    "           \n",
    "            if y[index].shape[0] == 0:\n",
    "                tree[best_split_feature][best_feature_value] = y.iloc[np.argmax(y.value_counts())]\n",
    "            \n",
    "            \n",
    "            else:\n",
    "                candidate_features_copy = candidate_features.copy()\n",
    "                candidate_features_copy.remove(best_split_feature)\n",
    "                tree[best_split_feature][best_feature_value] = self.__createTree(X = X.loc[index], y = y[index], candidate_features = candidate_features_copy, cur_depth = cur_depth + 1)\n",
    "        return tree\n",
    "\n",
    "    def __select_best_split_feature(self, X, y, candidate_features):\n",
    "        \"\"\"\n",
    "        __select_best_split_feature():\n",
    "            paivate function of object\n",
    "            parameters:\n",
    "                y:labels of samples in current leaf node \n",
    "                    dtype:Series\n",
    "                candidate_features:\n",
    "            return:\n",
    "                1. The feature which has the biggest purity gain\n",
    "                2. the biggest purity gain\n",
    "        \"\"\"\n",
    "       \n",
    "        if self.criterion == \"id3\":\n",
    "            \n",
    "            init_purity = self.__calculate_purity(y)\n",
    "          \n",
    "            purity_gain = {}\n",
    "           \n",
    "            for feature in candidate_features:\n",
    "               \n",
    "                purity_fix_feature = 0\n",
    "                \n",
    "                groups = y.groupby(X[feature], axis = 0)\n",
    "                \n",
    "                for name, group in groups:\n",
    "                        \n",
    "                        purity_fix_feature += (group.shape[0] / y.shape[0])*self.__calculate_purity(group)\n",
    "                \n",
    "                purity_gain[feature] = init_purity - purity_fix_feature\n",
    "\n",
    "           \n",
    "            pruity_gain = pd.Series(purity_gain)\n",
    "            \n",
    "            return pruity_gain.index[np.argmax(pruity_gain)] , pruity_gain.max()\n",
    "\n",
    "        \n",
    "        if self.criterion == \"c4.5\":\n",
    "            init_purity_ratio = self.__calculate_purity(y)\n",
    "           \n",
    "            purity_ratio_gain = {}\n",
    "          \n",
    "            for feature in candidate_features:\n",
    "                \n",
    "                purity_ratio_fix_feature = 0\n",
    "                \n",
    "                feature_purity = 0\n",
    "                \n",
    "                groups = y.groupby(X[feature], axis = 0)\n",
    "               \n",
    "                for name, group in groups:\n",
    "                    \n",
    "                    purity_ratio_fix_feature += (group.shape[0] / y.shape[0])*self.__calculate_purity(group)\n",
    "                    \n",
    "                    feature_purity += group.shape[0] / y.shape[0]\n",
    "                \n",
    "                purity_ratio_gain[feature] = (init_purity_ratio - purity_ratio_fix_feature) / feature_purity\n",
    "            \n",
    "           \n",
    "            pruity_ratio_gain = pd.Series(purity_ratio_gain)\n",
    "            \n",
    "            return pruity_ratio_gain.index[np.argmax(pruity_ratio_gain)] , pruity_ratio_gain.max()\n",
    "\n",
    "\n",
    "    def __calculate_purity(self, y):\n",
    "        \"\"\"\n",
    "        y ï¼š The label set which you want to calculate the purity of it. \n",
    "            dtype: Series\n",
    "        \"\"\"\n",
    "       \n",
    "        prob =  y.value_counts()\n",
    "       \n",
    "        if self.criterion == \"id3\" or \"c4.5\":\n",
    "            \n",
    "            return prob.apply(lambda x: -(x/y.shape[0])*np.log2(x/y.shape[0])).sum()\n",
    "\n",
    "    def __predict_single_sample(self, x):\n",
    "        \"\"\"\n",
    "        __predict_single_sample():\n",
    "            This function is used to priedict unknown single sample by tree which has already trained.\n",
    "            private function of object.\n",
    "            parameters:\n",
    "                x: the unknown sample which you want to predict.\n",
    "        \"\"\"\n",
    "        tree = self.tree\n",
    "        while isinstance(tree, dict):\n",
    "           \n",
    "            feature = list(tree.keys())[0]\n",
    "            \n",
    "            tree = list(tree.values())[0].get(x[feature])\n",
    "        return tree\n",
    "            \n",
    "            \n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        predict():\n",
    "            Predict the labels of multiple samples at the same time (relying on __predict_single_sample)\n",
    "            parameters:\n",
    "                X: the unknown sample which you want to predict.\n",
    "                    dtype:DataFrame\n",
    "        \"\"\"\n",
    "       \n",
    "        predict_label = []\n",
    "        for _, row in X.iterrows():\n",
    "            predict_label.append(self.__predict_single_sample(row))\n",
    "        return pd.Series(predict_label, index = X.index)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        score():compute the accuracy of input X and y.the formula of accuracy as following\n",
    "                        accuracy = Î£I{\\hat{y} - y}\n",
    "        parameters:\n",
    "            X : feature matrix of samples.\n",
    "                dtype : DataFrame\n",
    "            y : corrsponding true labels.\n",
    "                dtype : Series\n",
    "        \"\"\"\n",
    "        y_predict = self.predict(X)\n",
    "        return (y_predict == y).sum() / y.shape[0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
